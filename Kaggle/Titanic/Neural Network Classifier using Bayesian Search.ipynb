{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, AlphaDropout, BatchNormalization\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from sklearn.preprocessing import Normalizer, QuantileTransformer\n",
    "from sklearn.metrics import classification_report\n",
    "from time import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function to get and process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataget(train_path, test_path):\n",
    "\n",
    "    train_data = pd.read_csv(train_path)\n",
    "    test_data = pd.read_csv(test_path)\n",
    "    #Join the train and test data to cleanse and enhance the data\n",
    "    df = train_data.append(test_data, ignore_index=True)\n",
    "    Titles_Dictionary = {\n",
    "                        \"Capt\":         \"Officer\",\n",
    "                        \"Col\":          \"Officer\",\n",
    "                        \"Major\":        \"Officer\",\n",
    "                        \"Jonkheer\":     \"Royalty\",\n",
    "                        \"Don\":          \"Royalty\",\n",
    "                        \"Sir\":          \"Royalty\",\n",
    "                        \"Dr\":           \"Officer\",\n",
    "                        \"Rev\":          \"Officer\",\n",
    "                        \"the Countess\": \"Royalty\",\n",
    "                        \"Dona\":         \"Royalty\",\n",
    "                        \"Mme\":          \"Mrs\",\n",
    "                        \"Mlle\":         \"Miss\",\n",
    "                        \"Ms\":           \"Mrs\",\n",
    "                        \"Mr\":           \"Mr\",\n",
    "                        \"Mrs\":          \"Mrs\",\n",
    "                        \"Miss\":         \"Miss\",\n",
    "                        \"Master\":       \"Master\",\n",
    "                        \"Lady\":         \"Royalty\"\n",
    "                        }\n",
    "    ## Extract Title and map to the Titles from each Name\n",
    "    df['Title'] = df['Name'].apply(lambda x: Titles_Dictionary[x.split(',')[1].split('.')[0].strip()])\n",
    "    ## Fill missing Embarked with 'C'\n",
    "    df['Embarked'].fillna('C', inplace=True)\n",
    "    ## Note down the Imputed Ages\n",
    "    df['Imputed'] = df['Age'].isnull().astype('uint8')\n",
    "    columns = ['Age','Fare']\n",
    "    groups = ['Title', 'Embarked']\n",
    "    ## Fill null Ages with the mean Age based on Title, Embarked\n",
    "    df[columns] = df.groupby(groups)[columns].transform(lambda x: x.fillna(x.mean()))\n",
    "    ## Convert to categorical data\n",
    "    categories = ['Title', 'Sex', 'Pclass', 'SibSp', 'Parch', 'Embarked']\n",
    "    df[categories] = df[categories].apply(lambda x: x.astype('category'))\n",
    "    df = df.drop(columns=['Cabin', 'Name', 'Ticket'])\n",
    "    #df = df.drop(columns=['Title', 'SibSp', 'Imputed', 'Pclass', 'Parch', 'Embarked', 'Fare'])\n",
    "    df = df.round(2)\n",
    "    original = df.copy()\n",
    "    df = pd.get_dummies(df, drop_first=True)\n",
    "    test_data = df[df.Survived.isnull()].copy()\n",
    "    test_data = test_data.drop(columns=['Survived'])\n",
    "    train_data = df.dropna().copy()\n",
    "    train_data['Survived'] = train_data['Survived'].astype('uint8')\n",
    "    train_data = train_data.drop(columns=['PassengerId'])\n",
    "\n",
    "    return original, train_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(optimizer=Adam(amsgrad=True),\n",
    "                total_features=1,\n",
    "                activation1='elu',\n",
    "                activation2='elu',\n",
    "                units1=1,\n",
    "                units2=1,\n",
    "                dropout1=0.3,\n",
    "                dropout2=0.3,\n",
    "                multi_layer=False,\n",
    "                op_activation='sigmoid',\n",
    "                loadprevmodel=False,\n",
    "                modelname='Titanic-Kaggle-best'\n",
    "               ):\n",
    "    if loadprevmodel:\n",
    "        try:\n",
    "            model = load_model(modelname + '.h5')\n",
    "            print('Model loaded successfully')\n",
    "        except IOError:\n",
    "            print('Loading previous model failed, Building a new model')       \n",
    "    model = Sequential()\n",
    "    model.add(Dense(input_dim=total_features, activation=activation1, units=units1))\n",
    "    if activation1 == 'selu':\n",
    "        model.add(AlphaDropout(dropout1))\n",
    "    else:\n",
    "        model.add(BatchNormalization())\n",
    "    if multi_layer:\n",
    "        model.add(Dense(activation=activation2, units=units2))\n",
    "        if activation2 == 'selu':\n",
    "            model.add(AlphaDropout(dropout2))\n",
    "        else:\n",
    "            model.add(BatchNormalization())\n",
    "    model.add(Dense(units=1, activation=op_activation))\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Age Embarked   Fare Parch Pclass     Sex SibSp  Survived   Title  Imputed\n",
      "0  22.00        S   7.25     0      3    male     1       0.0      Mr        0\n",
      "1  38.00        C  71.28     0      1  female     1       1.0     Mrs        0\n",
      "2  26.00        S   7.92     0      3  female     0       1.0    Miss        0\n",
      "3  35.00        S  53.10     0      1  female     1       1.0     Mrs        0\n",
      "4  35.00        S   8.05     0      3    male     0       0.0      Mr        0\n",
      "5  36.24        Q   8.46     0      3    male     0       0.0      Mr        1\n",
      "6  54.00        S  51.86     0      1    male     0       0.0      Mr        0\n",
      "7   2.00        S  21.08     1      3    male     3       0.0  Master        0\n",
      "8  27.00        S  11.13     2      3  female     0       1.0     Mrs        0\n",
      "9  14.00        C  30.07     0      2  female     1       1.0     Mrs        0\n"
     ]
    }
   ],
   "source": [
    "train_path = 'train.csv'\n",
    "test_path = 'test.csv'\n",
    "original, train_data, test_data = dataget(train_path, test_path)\n",
    "df = original.copy()\n",
    "df.drop(columns=['PassengerId'], inplace=True)\n",
    "#df.dropna(inplace=True)\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Age         Fare    Survived      Imputed\n",
      "count  1309.000000  1309.000000  891.000000  1309.000000\n",
      "mean     30.085829    33.285921    0.383838     0.200917\n",
      "std      13.214767    51.740153    0.486592     0.400839\n",
      "min       0.170000     0.000000    0.000000     0.000000\n",
      "25%      22.000000     7.900000    0.000000     0.000000\n",
      "50%      30.000000    14.450000    0.000000     0.000000\n",
      "75%      36.240000    31.280000    1.000000     0.000000\n",
      "max      80.000000   512.330000    1.000000     1.000000\n"
     ]
    }
   ],
   "source": [
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1309 entries, 0 to 1308\n",
      "Data columns (total 10 columns):\n",
      "Age         1309 non-null float64\n",
      "Embarked    1309 non-null category\n",
      "Fare        1309 non-null float64\n",
      "Parch       1309 non-null category\n",
      "Pclass      1309 non-null category\n",
      "Sex         1309 non-null category\n",
      "SibSp       1309 non-null category\n",
      "Survived    891 non-null float64\n",
      "Title       1309 non-null category\n",
      "Imputed     1309 non-null uint8\n",
      "dtypes: category(6), float64(3), uint8(1)\n",
      "memory usage: 40.9 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = True\n",
    "modelh5 = 'Titanic-Kaggle'\n",
    "loadmodelh5 = 'Titanic-Kaggle-best'\n",
    "batch_size = 891\n",
    "epochs = 500\n",
    "normalizer = Normalizer(norm='l1')\n",
    "df = train_data\n",
    "train = df.dropna()\n",
    "y_train = train['Survived'].values\n",
    "x_train = train.drop(columns=['Survived']).values\n",
    "quantile_transformer = QuantileTransformer(output_distribution='normal')\n",
    "X_train = normalizer.fit_transform(x_train)\n",
    "#X_train = quantile_transformer.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "[CV] activation1=elu, activation2=selu, dropout1=0.052906988203063256, dropout2=0.08099717919844422, multi_layer=True, units1=43, units2=3 \n",
      "[CV]  activation1=elu, activation2=selu, dropout1=0.052906988203063256, dropout2=0.08099717919844422, multi_layer=True, units1=43, units2=3, total=  21.6s\n",
      "[CV] activation1=elu, activation2=selu, dropout1=0.052906988203063256, dropout2=0.08099717919844422, multi_layer=True, units1=43, units2=3 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   21.6s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  activation1=elu, activation2=selu, dropout1=0.052906988203063256, dropout2=0.08099717919844422, multi_layer=True, units1=43, units2=3, total=  13.1s\n",
      "[CV] activation1=elu, activation2=selu, dropout1=0.052906988203063256, dropout2=0.08099717919844422, multi_layer=True, units1=43, units2=3 \n",
      "[CV]  activation1=elu, activation2=selu, dropout1=0.052906988203063256, dropout2=0.08099717919844422, multi_layer=True, units1=43, units2=3, total=  13.9s\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "[CV] activation1=relu, activation2=selu, dropout1=0.4973597316233419, dropout2=0.07777426393068664, multi_layer=False, units1=37, units2=2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:   48.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  activation1=relu, activation2=selu, dropout1=0.4973597316233419, dropout2=0.07777426393068664, multi_layer=False, units1=37, units2=2, total=  13.3s\n",
      "[CV] activation1=relu, activation2=selu, dropout1=0.4973597316233419, dropout2=0.07777426393068664, multi_layer=False, units1=37, units2=2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   13.3s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  activation1=relu, activation2=selu, dropout1=0.4973597316233419, dropout2=0.07777426393068664, multi_layer=False, units1=37, units2=2, total=  12.8s\n",
      "[CV] activation1=relu, activation2=selu, dropout1=0.4973597316233419, dropout2=0.07777426393068664, multi_layer=False, units1=37, units2=2 \n",
      "[CV]  activation1=relu, activation2=selu, dropout1=0.4973597316233419, dropout2=0.07777426393068664, multi_layer=False, units1=37, units2=2, total=  14.1s\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "[CV] activation1=selu, activation2=elu, dropout1=0.5200459587240487, dropout2=0.39672089932716403, multi_layer=True, units1=31, units2=3 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:   40.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  activation1=selu, activation2=elu, dropout1=0.5200459587240487, dropout2=0.39672089932716403, multi_layer=True, units1=31, units2=3, total=  15.1s\n",
      "[CV] activation1=selu, activation2=elu, dropout1=0.5200459587240487, dropout2=0.39672089932716403, multi_layer=True, units1=31, units2=3 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   15.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  activation1=selu, activation2=elu, dropout1=0.5200459587240487, dropout2=0.39672089932716403, multi_layer=True, units1=31, units2=3, total=  17.5s\n",
      "[CV] activation1=selu, activation2=elu, dropout1=0.5200459587240487, dropout2=0.39672089932716403, multi_layer=True, units1=31, units2=3 \n",
      "[CV]  activation1=selu, activation2=elu, dropout1=0.5200459587240487, dropout2=0.39672089932716403, multi_layer=True, units1=31, units2=3, total=  15.7s\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "[CV] activation1=selu, activation2=elu, dropout1=0.6531573311338476, dropout2=0.10459908288331234, multi_layer=False, units1=28, units2=5 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:   48.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  activation1=selu, activation2=elu, dropout1=0.6531573311338476, dropout2=0.10459908288331234, multi_layer=False, units1=28, units2=5, total=  12.1s\n",
      "[CV] activation1=selu, activation2=elu, dropout1=0.6531573311338476, dropout2=0.10459908288331234, multi_layer=False, units1=28, units2=5 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   12.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  activation1=selu, activation2=elu, dropout1=0.6531573311338476, dropout2=0.10459908288331234, multi_layer=False, units1=28, units2=5, total=  12.2s\n",
      "[CV] activation1=selu, activation2=elu, dropout1=0.6531573311338476, dropout2=0.10459908288331234, multi_layer=False, units1=28, units2=5 \n",
      "[CV]  activation1=selu, activation2=elu, dropout1=0.6531573311338476, dropout2=0.10459908288331234, multi_layer=False, units1=28, units2=5, total=  12.4s\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "[CV] activation1=selu, activation2=elu, dropout1=0.09232377729929604, dropout2=0.19967343619055325, multi_layer=True, units1=31, units2=4 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:   36.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  activation1=selu, activation2=elu, dropout1=0.09232377729929604, dropout2=0.19967343619055325, multi_layer=True, units1=31, units2=4, total=  16.7s\n",
      "[CV] activation1=selu, activation2=elu, dropout1=0.09232377729929604, dropout2=0.19967343619055325, multi_layer=True, units1=31, units2=4 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   16.7s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  activation1=selu, activation2=elu, dropout1=0.09232377729929604, dropout2=0.19967343619055325, multi_layer=True, units1=31, units2=4, total=  17.3s\n",
      "[CV] activation1=selu, activation2=elu, dropout1=0.09232377729929604, dropout2=0.19967343619055325, multi_layer=True, units1=31, units2=4 \n",
      "[CV]  activation1=selu, activation2=elu, dropout1=0.09232377729929604, dropout2=0.19967343619055325, multi_layer=True, units1=31, units2=4, total=  20.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:   54.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BayesSearchCV took 247.54 seconds for 35 candidates parameter settings.\n",
      "val. score: 0.8035914702581369\n",
      "test score: 0.8462401795735129\n",
      "{'activation1': 'relu', 'activation2': 'selu', 'dropout1': 0.4973597316233419, 'dropout2': 0.07777426393068664, 'multi_layer': False, 'units1': 37, 'units2': 2}\n"
     ]
    }
   ],
   "source": [
    "params_dict ={'dropout1': Real(0.01, 1.0, 'log-uniform'),\n",
    "              'dropout2': Real(0.01, 1.0, 'log-uniform'),\n",
    "              'units1': Integer(int(X_train.shape[1]/2), int(X_train.shape[1] * 2)),\n",
    "              'units2': Integer(1,5),\n",
    "              'activation1': Categorical(['elu', 'relu', 'selu']),\n",
    "              'activation2': Categorical(['elu', 'relu', 'selu']),\n",
    "              'multi_layer': Categorical([True,False])\n",
    "        }\n",
    "if search:\n",
    "    random_search = BayesSearchCV(estimator=KerasClassifier(build_model,\n",
    "                                                            total_features=X_train.shape[1],\n",
    "                                                            batch_size=batch_size,\n",
    "                                                            epochs=epochs,\n",
    "                                                            verbose=0\n",
    "                                                           ),\n",
    "                                  search_spaces=params_dict,\n",
    "                                  scoring='accuracy',\n",
    "                                  n_iter=5,\n",
    "                                  cv=3,\n",
    "                                  verbose=2\n",
    "                                 )\n",
    "\n",
    "    start = time()\n",
    "    random_search.fit(X_train, y_train)\n",
    "    print(\"BayesSearchCV took %.2f seconds for %d candidates\"\n",
    "          \" parameter settings.\" % ((time() - start), random_search.total_iterations))\n",
    "\n",
    "    print(\"val. score: %s\" % random_search.best_score_)\n",
    "    print(\"test score: %s\" % random_search.score(X_train, y_train))\n",
    "    print(random_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(random_search.best_score_)\n",
    "print(random_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Docs\n",
    "#### http://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py\n",
    "### Bayes Search CV docs\n",
    "#### https://github.com/scikit-optimize/scikit-optimize/blob/master/skopt/searchcv.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params = {'units': 20, 'multi_layer': False, 'dropout_value': 0.31, 'activation': 'relu'} quantile\n",
    "#params = {'activation1': 'selu', 'activation2': 'selu', 'dropout1': 0.02291981150671515, 'dropout2': 0.12085674425994893, 'multi_layer': True, 'units1': 13, 'units2': 4}\n",
    "#params = {'activation1': 'relu', 'activation2': 'selu', 'dropout1': 0.2181680149312751, 'dropout2': 0.3136958264249214, 'multi_layer': False, 'units1': 14, 'units2': 3}\n",
    "#params = {'activation1': 'selu', 'activation2': 'elu', 'dropout1': 0.07484567296016119, 'dropout2': 0.020457806916385615, 'multi_layer': True, 'units1': 46, 'units2': 2}\n",
    "params = {'activation1': 'relu', 'activation2': 'selu', 'dropout1': 0.4973597316233419, 'dropout2': 0.07777426393068664, 'multi_layer': False, 'units1': 37, 'units2': 2}\n",
    "clf = KerasClassifier(build_model,\n",
    "                      total_features=X_train.shape[1],\n",
    "                      batch_size=batch_size,\n",
    "                      epochs=800,\n",
    "                      verbose=0,\n",
    "                     **params)\n",
    "clf.fit(X_train, y_train)\n",
    "y_true, y_pred = y_train, clf.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.98      0.88       549\n",
      "          1       0.94      0.59      0.73       342\n",
      "\n",
      "avg / total       0.85      0.83      0.82       891\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#params = {'activation1': 'relu', 'activation2': 'selu', 'dropout1': 0.4973597316233419, 'dropout2': 0.07777426393068664, 'multi_layer': False, 'units1': 37, 'units2': 2}\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = test_data\n",
    "resultdf = pd.DataFrame(data=df['PassengerId'])\n",
    "df = df.drop(columns=['PassengerId'])\n",
    "#test_x = quantile_transformer.transform(df)\n",
    "test_x = normalizer.transform(df)\n",
    "predictions = clf.predict(test_x)\n",
    "resultdf['Survived'] = predictions.astype(int)\n",
    "\n",
    "resultdf.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
